{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  1\n",
      "0  a stirring , funny and finally transporting re...  1\n",
      "1  apparently reassembled from the cutting room f...  0\n",
      "2  they presume their audience wo n't sit still f...  0\n",
      "3  this is a visually stunning rumination on love...  1\n",
      "4  jonathan parker 's bartleby should have been t...  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv('samples/train.tsv', delimiter='\\t', header=None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained bert components\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n"
     ]
    }
   ],
   "source": [
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([101, 1037, 18385, 1010, 6057, 1998, 2633, 18276, 2128, 16603, 1997, 5053, 1998, 1996, 6841, 1998, 5687, 5469, 3152, 102])\n",
      " list([101, 4593, 2128, 27241, 23931, 2013, 1996, 6276, 2282, 2723, 1997, 2151, 2445, 12217, 7815, 102])\n",
      " list([101, 2027, 3653, 23545, 2037, 4378, 24185, 1050, 1005, 1056, 4133, 2145, 2005, 1037, 11507, 10800, 1010, 2174, 14036, 2135, 3591, 1010, 2061, 2027, 19817, 4140, 2041, 1996, 7511, 2671, 4349, 3787, 1997, 11829, 7168, 9219, 1998, 28971, 2308, 1999, 8301, 8737, 2100, 4253, 102])\n",
      " ...\n",
      " list([101, 1996, 5896, 4472, 4121, 1010, 3082, 7832, 1999, 1037, 20857, 1010, 3302, 2100, 2126, 2008, 2515, 1050, 1005, 1056, 3749, 2151, 12369, 2046, 2339, 1010, 2005, 6013, 1010, 2204, 2477, 4148, 2000, 2919, 2111, 102])\n",
      " list([101, 1037, 5667, 2919, 2143, 2007, 5667, 25618, 7961, 2011, 3213, 2472, 9679, 15536, 15810, 2012, 1996, 9000, 2504, 102])\n",
      " list([101, 1037, 12090, 2135, 2512, 5054, 19570, 2389, 4038, 2055, 1037, 2103, 2746, 4237, 2012, 2049, 25180, 2015, 102])]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
